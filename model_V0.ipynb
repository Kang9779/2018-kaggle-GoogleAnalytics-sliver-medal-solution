{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fields\n",
    "- fullVisitorId- A unique identifier for each user of the Google Merchandise Store.\n",
    "\n",
    "- channelGrouping - The channel via which the user came to the Store.\n",
    "\n",
    "- date - The date on which the user visited the Store.\n",
    "\n",
    "- device - The specifications for the device used to access the Store.\n",
    "\n",
    "- geoNetwork - This section contains information about the geography of the user.\n",
    "\n",
    "- sessionId - A unique identifier for this visit to the store.\n",
    "\n",
    "- socialEngagementType - Engagement type, either \"Socially Engaged\" or \"Not Socially Engaged\".\n",
    "\n",
    "- totals - This section contains aggregate values across the session.\n",
    "\n",
    "- trafficSource - This section contains information about the Traffic Source from which the session originated.\n",
    "\n",
    "- visitId - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.\n",
    "\n",
    "- visitNumber - The session number for this user. If this is the first session, then this is set to 1.\n",
    "\n",
    "- visitStartTime - The timestamp (expressed as POSIX time).\n",
    "\n",
    "### Removed Data Fields\n",
    "Some fields were censored to remove target leakage. The major censored fields are listed below.\n",
    "\n",
    "- hits - This row and nested fields are populated for any and all types of hits. Provides a record of all page visits.\n",
    "\n",
    "- customDimensions - This section contains any user-level or session-level custom dimensions that are set for a session. This is a repeated field and has an entry for each dimension that is set.\n",
    "\n",
    "- totals - Multiple sub-columns were removed from the totals field.\n",
    "\n",
    "Data (116 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from load_df import *\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from plotly import tools\n",
    "\n",
    "py.init_notebook_mode(connected=True)\n",
    "color = sns.color_palette()\n",
    "feat_orignal_dir = \"./datasetsV2/data/\"\n",
    "submission_dir = 'submission/'\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 加载原始数据\n",
    "train_path = feat_orignal_dir + 'train_v2.csv'\n",
    "test_path = feat_orignal_dir + 'test_v2.csv'\n",
    "\n",
    "df_train,df_test = get_df(train_path,test_path)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(set(df_train.columns) - (set(df_train.columns) & set(df_test.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downCast_dtype(df):\n",
    "    '''\n",
    "        对数据类型进行转换\n",
    "    '''\n",
    "    float_cols = [c for c in df if df[c].dtype == 'float64']\n",
    "    int_cols = [c for c in df if df[c].dtype == 'int64']\n",
    "    \n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_const_cols(df_train,df_test):\n",
    "    #查询出具有列元素全部相同的列名\n",
    "    const_cols = [c for c in tqdm_notebook(df_test.columns) if df_test[c].nunique(dropna=False)==1 and c!='totals.visits']\n",
    "    if len(const_cols):\n",
    "        dropCols = const_cols +['trafficSource.adwordsClickInfo.page']+['totals.totalTransactionRevenue']+['totals.transactions']\n",
    "        df_test = df_test.drop(dropCols,axis=1)\n",
    "        df_train = df_train.drop(dropCols+['trafficSource.campaignCode'],axis=1)\n",
    "    return df_train,df_test\n",
    "\n",
    "df_train,df_test = remove_const_cols(df_train,df_test)\n",
    "df_train.sort_values(['date'],ascending=True,inplace=True)\n",
    "# df_train['totals.transactionRevenue'] = df_train['totals.transactionRevenue'].apply(lambda x:np.log1p(float(x)) if float(x) > 0 else 0)\n",
    "df_train['totals.transactionRevenue'] = df_train['totals.transactionRevenue'].fillna(0)\n",
    "df_test.sort_values(['date'],ascending=True,inplace=True)\n",
    "# df_test['totals.transactionRevenue'] = df_test['totals.transactionRevenue'].apply(lambda x:np.log1p(float(x)) if float(x) > 0 else 0)\n",
    "df_test['totals.transactionRevenue'] = df_test['totals.transactionRevenue'].fillna(0)\n",
    "df_test['is_test'] = True\n",
    "df_train['is_test'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data = pd.concat((df_train,df_test))\n",
    "df_data = downCast_dtype(df_data)\n",
    "\n",
    "print(df_train.shape,df_test.shape)\n",
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import date,timedelta\n",
    "\n",
    "def getDateFeatures(df):\n",
    "    \n",
    "    holiday = ['01-01','02-01','02-12','02-14','02-22','03-07','03-17','04-01','05-01','05-14',\n",
    "               '06-01','06-21','06-14','07-01','08-01','09-01','10-01','10-31','11-01','11-11',\n",
    "               '12-24','12-25']\n",
    "    df['visitStartTime'] = pd.to_datetime(df['visitStartTime'],unit='s')\n",
    "#     df['visitStartTime'] = pd.to_datetime(df['date'])\n",
    "    df['month'] = df['visitStartTime'].dt.month\n",
    "    df['week'] = df['visitStartTime'].dt.week\n",
    "    df['day'] = df['visitStartTime'].dt.day\n",
    "    df[\"weekOfday\"] = df['visitStartTime'].dt.weekday\n",
    "    for h in holiday:\n",
    "        h_month,h_day = h.split('-')\n",
    "        df['is_holiday'] = ((df['month']==int(h_month))&(df['day']==int(h_day))|(df['weekOfday']==5)|(df['weekOfday']==6))*1\n",
    "\n",
    "    #某日前两天、和后两天是否为假期\n",
    "    df['prev_day_is_holiday'] = df['is_holiday'].shift().fillna(0)\n",
    "    df['pre2_day_is_holiday'] = df['is_holiday'].shift(2).fillna(0)\n",
    "    df['next_day_is_holiday'] = df['is_holiday'].shift(-1).fillna(0)\n",
    "    df['next2_day_is_holiday'] = df['is_holiday'].shift(-2).fillna(0)\n",
    "\n",
    "    df['month.unique.user.count'] = df.groupby('month')['fullVisitorId'].transform('nunique')\n",
    "    df['day.unique.user.count'] = df.groupby('day')['fullVisitorId'].transform('nunique')\n",
    "    df['weekday.unique.user.count'] = df.groupby('weekOfday')['fullVisitorId'].transform('nunique')\n",
    "\n",
    "    df['next_session_1'] = (\n",
    "        df['visitStartTime'] - df[['fullVisitorId', 'visitStartTime']].groupby('fullVisitorId')['visitStartTime'].shift(1)\n",
    "    ).astype(np.int64) // 1e9 // 60 // 60\n",
    "    df['next_session_2'] = (\n",
    "        df['visitStartTime'] - df[['fullVisitorId', 'visitStartTime']].groupby('fullVisitorId')['visitStartTime'].shift(-1)\n",
    "    ).astype(np.int64) // 1e9 // 60 // 60\n",
    "\n",
    "    days_of_months = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    #当天距离月初月末的距离\n",
    "    df[\"days_to_side\"] = df['visitStartTime'].apply(\n",
    "                            lambda x: min(x.day, days_of_months[x.month-1]-x.day))\n",
    "    df['day'] = df['day'].apply(lambda x:0 if x<=7 else 2 if x>=24 else 1)\n",
    "    df.drop(columns=['visitStartTime'],axis=1,inplace=True)\n",
    "    \n",
    "    return df\n",
    "df_data = getDateFeatures(df_data)\n",
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getLagFeatures(df, time_col, group_cols, shifts):    \n",
    "    '''\n",
    "    For epoch time, compute deltas with the specified shift on sequences\n",
    "    aggregated by group_cols, return df with new columns\n",
    "    '''\n",
    "    df = df.sort_values(by=time_col)\n",
    "    for shift in shifts:\n",
    "        feat_name = '_'.join(group_cols) + ('_delta_shift_%d' % shift) \n",
    "        \n",
    "        df[feat_name] = (df.groupby(group_cols)\n",
    "                            [time_col].shift(shift) - df[time_col]).astype(np.float32)\n",
    "        df[feat_name] = df[feat_name] * -1 * np.sign(shift) # flip sign for lags\n",
    "        df[feat_name] = df[feat_name].fillna(0)\n",
    "    return df\n",
    "\n",
    "lags = [x for x in range(-6,7) if x!=0]\n",
    "\n",
    "# for df in [train_df,test_df]:\n",
    "#     df = add_grouped_time_delta_features(df,'visitStartTime',['fullVisitorId'],lags)\n",
    "#     df = add_grouped_time_delta_features(df,'totals.visits',['fullVisitorId','month','week_day'],lags)\n",
    "#     df = add_grouped_time_delta_features(df,'totals.visits',['fullVisitorId','month','day'],lags)\n",
    "#     df = add_grouped_time_delta_features(df,'totals.visits',['fullVisitorId','month','hour'],lags)\n",
    "#     df = add_grouped_time_delta_features(df,'totals.visits',['fullVisitorId','month'],lags)\n",
    "\n",
    "#     df = add_grouped_time_delta_features(df,'totals.hits',['fullVisitorId','month','week_day'],lags)\n",
    "#     df = add_grouped_time_delta_features(df,'totals.hits',['fullVisitorId','month','day'],lags)\n",
    "#     df = add_grouped_time_delta_features(df,'totals.hits',['fullVisitorId','month','hour'],lags)\n",
    "#     df = add_grouped_time_delta_features(df,'totals.hits',['fullVisitorId','month'],lags)\n",
    "\n",
    "#     df = add_grouped_time_delta_features(df,'totals.pageviews',['fullVisitorId','month','week_day'],lags)\n",
    "#     df = add_grouped_time_delta_features(df,'totals.pageviews',['fullVisitorId','month','day'],lags)\n",
    "#     df = add_grouped_time_delta_features(df,'totals.pageviews',['fullVisitorId','month','hour'],lags)\n",
    "#     df = add_grouped_time_delta_features(df,'totals.pageviews',['fullVisitorId','month'],lags)\n",
    "\n",
    "# print(train_df.shape)\n",
    "# print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getTotalFeatures(df):\n",
    "    '''\n",
    "        totals点击量处理\n",
    "    '''\n",
    "    df['totals.pageviews'] = df['totals.pageviews'].fillna(1.0)\n",
    "    df['totals.newVisits'] = df['totals.newVisits'].fillna(0)\n",
    "    df['totals.bounces'] = df['totals.bounces'].fillna(0)\n",
    "    \n",
    "    #用户每天，每周，每月的访问量\n",
    "    df['sum_visits_per_day_visitor'] = df.groupby(['fullVisitorId','month','day'])['totals.visits'].transform('sum')\n",
    "    df['sum_visits_per_week_visitor'] = df.groupby(['fullVisitorId','month','weekOfday'])['totals.visits'].transform('sum')\n",
    "    df['sum_visits_per_month_visitor'] = df.groupby(['fullVisitorId','month'])['totals.visits'].transform('sum')\n",
    "    \n",
    "    #用户每天，每周，每月的点击量\n",
    "    df['sum_hits_per_day_visitor'] = df.groupby(['fullVisitorId','month','day'])['totals.hits'].transform('sum')\n",
    "    df['sum_hits_per_week_visitor'] = df.groupby(['fullVisitorId','month','weekOfday'])['totals.hits'].transform('sum')\n",
    "    df['sum_hits_per_month_visitor'] = df.groupby(['fullVisitorId','month'])['totals.hits'].transform('sum')\n",
    "    \n",
    "    #用户每天，每周，每月的网页浏览量\n",
    "    df['sum_pageviews_per_day_visitor'] = df.groupby(['fullVisitorId','month','day'])['totals.pageviews'].transform('sum')\n",
    "    df['sum_pageviews_per_week_visitor'] = df.groupby(['fullVisitorId','month','weekOfday'])['totals.pageviews'].transform('sum')\n",
    "    df['sum_pageviews_per_month_visitor'] = df.groupby(['fullVisitorId','month'])['totals.pageviews'].transform('sum')\n",
    "    \n",
    "    #用户每个pageView产生的点击量hits/pageviews\n",
    "    df['hits/pageViews_per_day_visitor'] = df['sum_hits_per_day_visitor'].values/df['sum_pageviews_per_day_visitor'].values\n",
    "    df['hits/pageViews_per_week_visitor'] = df['sum_hits_per_week_visitor'].values/df['sum_pageviews_per_week_visitor'].values\n",
    "    df['hits/pageviews_per_month_visitor'] =df['sum_hits_per_month_visitor'].values/df['sum_pageviews_per_month_visitor'].values\n",
    "    \n",
    "    #用户每天，每周，每月的session量\n",
    "    df['sum_visitNumber_per_day_visitor'] = df.groupby(['fullVisitorId','month','day'])['visitNumber'].transform('sum')\n",
    "    df['sum_visitNumber_per_week_visitor'] = df.groupby(['fullVisitorId','month','weekOfday'])['visitNumber'].transform('sum')\n",
    "    df['sum_visitNumber_per_month_visitor'] = df.groupby(['fullVisitorId','month'])['visitNumber'].transform('sum')\n",
    "    \n",
    "    #平均session量\n",
    "    df['mean_vistiNumber_per_day_visitor'] = df.groupby(['fullVisitorId','month','day'])['visitNumber'].transform('mean')\n",
    "    df['mean_visitNumebr_per_week_visitor']= df.groupby(['fullVisitorId','month','weekOfday'])['visitNumber'].transform('mean')\n",
    "    df['mean_visitNumber_per_month_visitor'] = df.groupby(['fullVisitorId','month'])['visitNumber'].transform('mean')\n",
    "    \n",
    "    #每次session产生的pageviews pageview/visitNumber\n",
    "    df['pageviews/visitNumber_per_day_visitor'] = df['sum_pageviews_per_day_visitor'].values/df['sum_visitNumber_per_day_visitor'].values\n",
    "    df['pageviews/visitNumber_per_week_visitor'] = df['sum_pageviews_per_week_visitor'].values/df['sum_visitNumber_per_week_visitor'].values\n",
    "    df['pageviews/visitNumber_per_month_visitor'] = df['sum_pageviews_per_month_visitor'].values/df['sum_visitNumber_per_month_visitor'].values\n",
    "    \n",
    "    df['mean.hits.per.day'] = df.groupby(['month','day'])['totals.hits'].transform('mean')\n",
    "    df['sum.hits.per.day'] = df.groupby(['month','day'])['totals.hits'].transform('sum')\n",
    "    \n",
    "    df['mean.hits.per.month'] = df.groupby(['month'])['totals.hits'].transform('mean')\n",
    "    df['sum.hits.per.month'] = df.groupby(['month'])['totals.hits'].transform('sum')\n",
    "    \n",
    "    df['mean.hits.per.week_day'] = df.groupby(['month','weekOfday'])['totals.hits'].transform('mean')\n",
    "    df['sum.hits.per.week_day'] = df.groupby(['month','weekOfday'])['totals.hits'].transform('sum')\n",
    "    \n",
    "    df['mean.pageviews.per.day'] = df.groupby(['month','day'])['totals.pageviews'].transform('mean')\n",
    "    df['sum.pageviews.per.day'] = df.groupby(['month','day'])['totals.pageviews'].transform('sum')\n",
    "    \n",
    "    df['mean.pageviews.per.month'] = df.groupby(['month'])['totals.pageviews'].transform('mean')\n",
    "    df['sum.pageviews.per.month'] = df.groupby(['month'])['totals.pageviews'].transform('sum')\n",
    "    \n",
    "    return df\n",
    "df_data = getTotalFeatures(df_data)\n",
    "print(df_data.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#类别特征处理\n",
    "device_browers = list(df_data['device.browser'].value_counts().reset_index()['index'][0:30])\n",
    "device_os = list(df_data['device.operatingSystem'].value_counts().reset_index()['index'][0:15])\n",
    "\n",
    "geoNetwork_cities = list(df_data['geoNetwork.city'].value_counts().reset_index()['index'][0:10])\n",
    "geoNetwork_country = list(df_data['geoNetwork.country'].value_counts().reset_index()['index'][0:20])\n",
    "geoNetwork_metro = list(df_data['geoNetwork.metro'].value_counts().reset_index()['index'][0:40])\n",
    "geoNetwork_networkDomain = list(df_data['geoNetwork.networkDomain'].value_counts().reset_index()['index'][0:40])\n",
    "\n",
    "def browser_mapping(x):\n",
    "    if x in device_browers:\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return 'others'\n",
    "\n",
    "def geoNetwork_city(x):\n",
    "    if x in geoNetwork_cities:\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return 'others'\n",
    "    \n",
    "def geoNetwork_countries(x):\n",
    "    if x in geoNetwork_country:\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return 'others'\n",
    "\n",
    "def adcontents_mapping(x):\n",
    "    if  ('google' in x):\n",
    "        return 'google'\n",
    "    elif  ('placement' in x) | ('placememnt' in x):\n",
    "        return 'placement'\n",
    "    elif '(not set)' in x or 'nan' in x:\n",
    "        return x\n",
    "    elif 'ad' in x:\n",
    "        return 'ad'\n",
    "    else:\n",
    "        return 'others'\n",
    "    \n",
    "def device_operatingSystem(x):\n",
    "    if x in device_os:\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return 'others'\n",
    "\n",
    "def traficSource_referralPath(x):\n",
    "    if x == '/':\n",
    "        return '/'\n",
    "    elif 'yt/about/' in x:\n",
    "        return x\n",
    "    elif 'google' in x:\n",
    "        return '/google'\n",
    "    elif 'mail' in x:\n",
    "        return '/mail'\n",
    "    elif 'yt/advertise/' in x:\n",
    "        return x\n",
    "    elif 'offer/2145' in x:\n",
    "        return x\n",
    "    elif 'yt/creators/' in x:\n",
    "        return x\n",
    "    elif 'pagead/ads' in x:\n",
    "        return x\n",
    "    elif '/intl/' in x:\n",
    "        return x;\n",
    "    elif 'shirt' in x:\n",
    "        return x;\n",
    "    elif '/analytics/app/' in x:\n",
    "        return x\n",
    "    elif 'using-the-logo':\n",
    "        return x\n",
    "    elif '/moma' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return '/others'  \n",
    "    \n",
    "def source_mapping(x):\n",
    "    if  ('google' in x):\n",
    "        return 'google'\n",
    "    elif('(direct)'in x):\n",
    "        return x;\n",
    "    elif  ('youtube' in x):\n",
    "        return 'youtube'\n",
    "    elif '(not set)' in x or 'nan' in x:\n",
    "        return x\n",
    "    elif 'yahoo' in x:\n",
    "        return 'yahoo'\n",
    "    elif 'facebook' in x:\n",
    "        return 'facebook'\n",
    "    elif 'reddit' in x:\n",
    "        return 'reddit'\n",
    "    elif 'bing' in x:\n",
    "        return 'bing'\n",
    "    elif 'quora' in x:\n",
    "        return 'quora'\n",
    "    elif 'outlook' in x:\n",
    "        return 'outlook'\n",
    "    elif 'linkedin' in x:\n",
    "        return 'linkedin'\n",
    "    elif 'pinterest' in x:\n",
    "        return 'pinterest'\n",
    "    elif 'ask' in x:\n",
    "        return 'ask'\n",
    "    elif 'siliconvalley' in x:\n",
    "        return 'siliconvalley'\n",
    "    elif 'lunametrics' in x:\n",
    "        return 'lunametrics'\n",
    "    elif 'amazon' in x:\n",
    "        return 'amazon'\n",
    "    elif 'mysearch' in x:\n",
    "        return 'mysearch'\n",
    "    elif 'qiita' in x:\n",
    "        return 'qiita'\n",
    "    elif 'messenger' in x:\n",
    "        return 'messenger'\n",
    "    elif 'twitter' in x:\n",
    "        return 'twitter'\n",
    "    elif 't.co' in x:\n",
    "        return 't.co'\n",
    "    elif 'vk.com' in x:\n",
    "        return 'vk.com'\n",
    "    elif 'search' in x:\n",
    "        return 'search'\n",
    "    elif 'edu' in x:\n",
    "        return 'edu'\n",
    "    elif 'mail' in x:\n",
    "        return 'mail'\n",
    "    elif 'ad' in x:\n",
    "        return 'ad'\n",
    "    elif 'golang' in x:\n",
    "        return 'golang'\n",
    "    elif 'direct' in x:\n",
    "        return 'direct'\n",
    "    elif 'dealspotr' in x:\n",
    "        return 'dealspotr'\n",
    "    elif 'sashihara' in x:\n",
    "        return 'sashihara'\n",
    "    elif 'phandroid' in x:\n",
    "        return 'phandroid'\n",
    "    elif 'baidu' in x:\n",
    "        return 'baidu'\n",
    "    elif 'mdn' in x:\n",
    "        return 'mdn'\n",
    "    elif 'duckduckgo' in x:\n",
    "        return 'duckduckgo'\n",
    "    elif 'seroundtable' in x:\n",
    "        return 'seroundtable'\n",
    "    elif 'metrics' in x:\n",
    "        return 'metrics'\n",
    "    elif 'sogou' in x:\n",
    "        return 'sogou'\n",
    "    elif 'businessinsider' in x:\n",
    "        return 'businessinsider'\n",
    "    elif 'github' in x:\n",
    "        return 'github'\n",
    "    elif 'gophergala' in x:\n",
    "        return 'gophergala'\n",
    "    elif 'yandex' in x:\n",
    "        return 'yandex'\n",
    "    elif 'msn' in x:\n",
    "        return 'msn'\n",
    "    elif 'dfa' in x:\n",
    "        return 'dfa'\n",
    "    elif 'feedly' in x:\n",
    "        return 'feedly'\n",
    "    elif 'arstechnica' in x:\n",
    "        return 'arstechnica'\n",
    "    elif 'squishable' in x:\n",
    "        return 'squishable'\n",
    "    elif 'flipboard' in x:\n",
    "        return 'flipboard'\n",
    "    elif 't-online.de' in x:\n",
    "        return 't-online.de'\n",
    "    elif 'sm.cn' in x:\n",
    "        return 'sm.cn'\n",
    "    elif 'wow' in x:\n",
    "        return 'wow'\n",
    "    elif 'baidu' in x:\n",
    "        return 'baidu'\n",
    "    elif 'partners' in x:\n",
    "        return 'partners'\n",
    "    else:\n",
    "        return 'others'\n",
    "\n",
    "for df in [df_data]:  \n",
    "    df['device.browser'] = df['device.browser'].map(lambda x:browser_mapping(str(x))).astype('str')\n",
    "    df['device.operatingSystem'] = df['device.operatingSystem'].map(lambda x:device_operatingSystem(str(x))).astype('str')\n",
    "    df['trafficSource.adContent'] = df['trafficSource.adContent'].map(lambda x:adcontents_mapping(str(x).lower())).astype('str')\n",
    "    df['trafficSource.source'] = df['trafficSource.source'].map(lambda x:source_mapping(str(x).lower())).astype('str')\n",
    "    df['geoNetwork.city'] = df['geoNetwork.city'].map(lambda x:geoNetwork_city(str(x))).astype('str')\n",
    "    df['geoNetwork.country'] = df['geoNetwork.country'].map(lambda x:geoNetwork_countries(str(x))).astype('str')\n",
    "    df['trafficSource.referralPath'] = df['trafficSource.referralPath'].map(lambda x:traficSource_referralPath(str(x))).astype('str')\n",
    "\n",
    "#将具有从属关系的特征进行合并:地区从属关系，市场分区\n",
    "for df in [df_data]:\n",
    "    print(\"... process device ...\")\n",
    "    df['source.country'] = df['trafficSource.source'] + '_' + df['geoNetwork.country']\n",
    "    df['campaign.medium'] = df['trafficSource.campaign'] + '_' + df['trafficSource.medium']\n",
    "    df['browser.category'] = df['device.browser'] + '_' + df['device.deviceCategory']\n",
    "    df['browser.os'] = df['device.browser'] + '_' + df['device.operatingSystem']\n",
    "\n",
    "#将设备中具有从属关系的特征整合\n",
    "def getCustormFeature(df):\n",
    "    print('... custom ...')\n",
    "    df['device_deviceCategory_channelGrouping'] = df['device.deviceCategory'] + \"_\" + df['channelGrouping']\n",
    "    df['channelGrouping_browser'] = df['device.browser'] + \"_\" + df['channelGrouping']\n",
    "    df['channelGrouping_OS'] = df['device.operatingSystem'] + \"_\" + df['channelGrouping']\n",
    "    \n",
    "    df['city_continent_country_metro_networkDomain_region_subContinent'] = df['geoNetwork.city']+\"_\"+df['geoNetwork.continent']+\"_\"+df['geoNetwork.country']+\"_\"+df['geoNetwork.metro']+\"_\"+df['geoNetwork.networkDomain']+\"_\"+df['geoNetwork.region']+df['geoNetwork.subContinent']\n",
    "    \n",
    "    for i in ['geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country','geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region','geoNetwork.subContinent']:\n",
    "        for j in ['device.browser','device.deviceCategory', 'device.operatingSystem', 'trafficSource.source']:\n",
    "            df[i + \"_\" + j] = df[i] + \"_\" + df[j]\n",
    "    \n",
    "    df['content.source'] = df['trafficSource.adContent'] + \"_\" + df['source.country']\n",
    "    df['medium.source'] = df['trafficSource.medium'] + \"_\" + df['source.country']\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_data = getCustormFeature(df_data)\n",
    "\n",
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#设备&网络特征\n",
    "def getDeviceFeature(df,groups=None,feaCols=None):\n",
    "    feCols = []\n",
    "    for group in groups:\n",
    "        for fe in feaCols:\n",
    "            df['sum_%s_By%s'%(fe,group)] = df.groupby(group)[fe].transform('sum')\n",
    "            df['mean_%s_By%s'%(fe,group)] = df.groupby(group)[fe].transform('mean')\n",
    "            feCols.append('sum_%s_By%s'%(fe,group))\n",
    "            feCols.append('mean_%s_By%s'%(fe,group))\n",
    "    df.fillna(0,inplace=True)\n",
    "    return df,feCols\n",
    "deviceCols = []\n",
    "deviceGroups = ['device.browser','device.operatingSystem','device.isMobile','device.deviceCategory',\n",
    "                'geoNetwork.networkDomain']\n",
    "totalCols = ['totals.hits','totals.visits','totals.pageviews','totals.timeOnSite','totals.sessionQualityDim']\n",
    "df_data,deviceCols = getDeviceFeature(df_data,groups=deviceGroups,feaCols=totalCols)\n",
    "\n",
    "df_data = downCast_dtype(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "excluded_cols = ['fullVisitorId','date','totals.transactionRevenue','visitStartTime']\n",
    "cat_cols = [col for col in df_data.columns if df_data[col].dtype ==\"object\" and col not in excluded_cols]\n",
    "# label encode the categorical variables and convert the numerical variables to float\n",
    "def label_encoding(df_data):\n",
    "    for col in tqdm_notebook(cat_cols):\n",
    "        if df_data[col].dtype == bool:#将bool变量转成整数\n",
    "            df_data[col] = df_data[col].astype(int)\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(df_data[col].values.astype('str')))\n",
    "        \n",
    "        df_data[col] = lbl.transform(list(df_data[col].values.astype('str')))\n",
    "        \n",
    "    return df_data\n",
    "\n",
    "df_data = label_encoding(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征文件保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data.replace([np.inf,-np.inf],np.nan,inplace=True)\n",
    "df_data.fillna(0,inplace=True)\n",
    "\n",
    "df_train = df_data[df_data.is_test==0]\n",
    "df_test = df_data[df_data.is_test==1]\n",
    "df_train.to_csv('./features/'+'df_train.csv',index=None)\n",
    "df_test.to_csv('./features/'+'df_test.csv',index=None)\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def score_model(pred_val,val_df):\n",
    "    '''\n",
    "    input :\n",
    "        pred_val:numpy array\n",
    "        val_df:DataFrame \n",
    "    return :\n",
    "        score\n",
    "    '''\n",
    "    pred_val[pred_val<0] = 0\n",
    "    val_pred_df = pd.DataFrame({\"fullVisitorId\":val_df[\"fullVisitorId\"].values})\n",
    "\n",
    "    val_pred_df[\"transactionRevenue\"] = val_df[\"totals.transactionRevenue\"].values\n",
    "    val_pred_df[\"PredictedRevenue\"] = np.expm1(pred_val)\n",
    "\n",
    "    val_pred_df = val_pred_df.groupby(\"fullVisitorId\")[\"transactionRevenue\", \"PredictedRevenue\"].sum().reset_index()\n",
    "    \n",
    "    score = np.sqrt(metrics.mean_squared_error(np.log1p(val_pred_df[\"transactionRevenue\"].values), np.log1p(val_pred_df[\"PredictedRevenue\"].values)))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./features/'+'df_train.csv',dtype={'fullVisitorId': np.str,'date':np.str})\n",
    "df_test = pd.read_csv('./features/'+'df_test.csv',dtype={'fullVisitorId': np.str,'date':np.str})\n",
    "print(df_train.shape,df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification With GroupKFold\n",
    "   * lgbclf\n",
    "   * xgbclf\n",
    "   * catclf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "excluded_cols = ['date','fullVisitorId','totals.transactionRevenue','visitStartTime']\n",
    "tr_features = [_f for _f in df_train.columns if _f not in excluded_cols]\n",
    "train_label = (df_train['totals.transactionRevenue']>0).astype(np.uint8)\n",
    "train_y = df_train['totals.transactionRevenue'].apply(lambda x:np.log1p(float(x)) if float(x) > 0 else 0)\n",
    "\n",
    "#检查分类样本是否不平衡\n",
    "train_label.value_counts()/train_label.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as catb\n",
    "\n",
    "def get_folds(df=None, n_splits=5):\n",
    "    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n",
    "    # Get sorted unique visitors\n",
    "    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n",
    "\n",
    "    # Get folds\n",
    "    folds = GroupKFold(n_splits=n_splits)\n",
    "    fold_ids = []\n",
    "    ids = np.arange(df.shape[0])\n",
    "    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n",
    "        fold_ids.append(\n",
    "            [\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return fold_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#注意分类样本存在不平衡\n",
    "def clf_getOutFold(clf,x_train,y_train,x_test):\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    NFOLDS = 5\n",
    "    \n",
    "    folds = get_folds(df=x_train, n_splits=NFOLDS)\n",
    "    oof_train_pred = np.zeros((ntrain,2))\n",
    "    oof_test_pred = np.zeros((ntest,2))\n",
    "    oof_test_pred_skf = np.empty((ntest,2,NFOLDS))\n",
    "  \n",
    "    for i, (dev_index, val_index) in enumerate(folds):\n",
    "        x_dev = x_train[tr_features].iloc[dev_index]\n",
    "        y_dev = y_train.iloc[dev_index]\n",
    "        x_val = x_train[tr_features].iloc[val_index]\n",
    "        y_val = y_train.iloc[val_index]\n",
    "\n",
    "        clf.fit(x_dev, y_dev,eval_set=[(x_dev,y_dev),(x_val,y_val)],early_stopping_rounds=100,verbose=100)\n",
    "        \n",
    "        oof_test_pred_skf[:,:,i] = clf.predict_proba(x_test[tr_features],num_iteration=clf.best_iteration_)\n",
    "        oof_train_pred[val_index,:] = clf.predict_proba(x_val,num_iteration=clf.best_iteration_)\n",
    "            \n",
    "    oof_test_pred[:] = oof_test_pred_skf.mean(axis=-1)\n",
    "    \n",
    "    print(\"-\"*50+str(\"clf training done！\")+\"-\"*50)\n",
    "    \n",
    "    return clf,oof_test_pred,oof_train_pred\n",
    "\n",
    "lgb_clf_params = {\n",
    "    'num_leaves':31,\n",
    "    'learning_rate':0.03,\n",
    "    'n_estimators':5000,\n",
    "    'class_weight':'balanced',#平衡样本\n",
    "    'subsample':0.9,\n",
    "    'colsample_bytree':0.9,\n",
    "    'random_state':20,\n",
    "    'objective':'binary',\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id':1,\n",
    "    'gpu_device_id': 1,\n",
    "}\n",
    "lgb_clf = lgb.LGBMClassifier(**lgb_clf_params)\n",
    "\n",
    "lgb_clf,clf_test_pred,clf_train_pred = clf_getOutFold(lgb_clf,df_train,train_label,df_test)\n",
    "# pd.DataFrame(data=clf_test_pred,columns=['lgb_proba_0','lgb_proba_1'],dtype=np.float32).to_csv(feat_orignal_dir+'lgbclf_testpred.csv',index=None)\n",
    "# pd.DataFrame(data=clf_train_pred,columns=['lgb_proba_0','lgb_proba_1'],dtype=np.float32).to_csv(feat_orignal_dir+'lgbclf_trainpred.csv',index=None)\n",
    "#加入分类特征\n",
    "df_train['lgb_proba_0'] = clf_train_pred[:,0]\n",
    "df_train['lgb_proba_1'] = clf_train_pred[:,1]\n",
    "df_test['lgb_proba_0'] = clf_test_pred[:,0]\n",
    "df_test['lgb_proba_1'] = clf_test_pred[:,1]\n",
    "\n",
    "## XGB Classifier\n",
    "def clf_getOutFold2(clf,x_train,y_train,x_test):\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    NFOLDS = 10\n",
    "    \n",
    "    folds = get_folds(df=x_train, n_splits=NFOLDS)\n",
    "\n",
    "    oof_train_pred = np.zeros((ntrain,2))\n",
    "    oof_test_pred = np.zeros((ntest,2))\n",
    "    oof_test_pred_skf = np.empty((ntest,2,NFOLDS))\n",
    "\n",
    "    for i, (dev_index, val_index) in enumerate(folds):\n",
    "        x_dev = x_train[tr_features].iloc[dev_index]\n",
    "        y_dev = y_train.iloc[dev_index]\n",
    "        x_val = x_train[tr_features].iloc[val_index]\n",
    "        y_val = y_train.iloc[val_index]\n",
    "\n",
    "        clf.fit(x_dev, y_dev,eval_set=[(x_dev,y_dev),(x_val,y_val)],early_stopping_rounds=100,verbose=100)\n",
    "    \n",
    "        oof_test_pred_skf[:,:,i] = clf.predict_proba(x_test[tr_features])\n",
    "        oof_train_pred[val_index] = clf.predict_proba(x_val)\n",
    "        \n",
    "    oof_test_pred[:] = oof_test_pred_skf.mean(axis=-1)\n",
    "\n",
    "    print(\"-\"*50+str(\"clf training done！\")+\"-\"*50)\n",
    "    \n",
    "    return clf,oof_test_pred,oof_train_pred\n",
    "\n",
    "xgb_clf_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators':5000,\n",
    "    'scale_pos_weight':77.4,#平衡样本\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth': 22,\n",
    "    'min_child_weight': 57,\n",
    "    'subsample': 0.67,\n",
    "    'colsample_bytree': 0.054,\n",
    "    'colsample_bylevel': 0.50,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 20\n",
    "}\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(**xgb_clf_params)\n",
    "\n",
    "xgb_clf,clf_test_pred,clf_train_pred = clf_getOutFold2(xgb_clf,df_train,train_label,df_test)\n",
    "\n",
    "# pd.DataFrame(data=clf_test_pred,columns=['xgb_proba_0','xgb_proba_1'],dtype=np.float32).to_csv(feat_orignal_dir+'xgbclf_testpred.csv',index=None)\n",
    "# pd.DataFrame(data=clf_train_pred,columns=['xgb_proba_0','xgb_proba_1'],dtype=np.float32).to_csv(feat_orignal_dir+'xgbclf_trainpred.csv',index=None)\n",
    "\n",
    "df_train['xgb_proba_0'] = clf_train_pred[:,0]\n",
    "df_train['xgb_proba_1'] = clf_train_pred[:,1]\n",
    "df_test['xgb_proba_0'] = clf_test_pred[:,0]\n",
    "df_test['xgb_proba_1'] = clf_test_pred[:,1]\n",
    "\n",
    "### Catboost classifier\n",
    "import catboost as catb\n",
    "cat_clf_params = {\n",
    "    'n_estimators':5000,\n",
    "    'learning_rate':0.02,\n",
    "    'max_depth':10,\n",
    "    'scale_pos_weight':77.4,#平衡样本\n",
    "    'loss_function':'Logloss',\n",
    "    'eval_metric':'Logloss',\n",
    "    'random_state':20,\n",
    "    'bagging_temperature':0.2,\n",
    "    'od_type':'Iter',\n",
    "    'od_wait':20\n",
    "}\n",
    "cat_clf = catb.CatBoostClassifier(**cat_clf_params)\n",
    "\n",
    "cat_clf,clf_test_pred,clf_train_pred = clf_getOutFold2(cat_clf,df_train,train_label,df_test)\n",
    "\n",
    "# pd.DataFrame(data=clf_test_pred,columns=['cat_proba_0','cat_proba_1'],dtype=np.float32).to_csv(feat_orignal_dir+'catclf_testpred.csv',index=None)\n",
    "# pd.DataFrame(data=clf_train_pred,columns=['cat_proba_0','cat_proba_1'],dtype=np.float32).to_csv(feat_orignal_dir+'catclf_trainpred.csv',index=None)\n",
    "\n",
    "df_train['cat_proba_0'] = clf_train_pred[:,0]\n",
    "df_train['cat_proba_1'] = clf_train_pred[:,1]\n",
    "df_test['cat_proba_0'] = clf_test_pred[:,0]\n",
    "df_test['cat_proba_1'] = clf_test_pred[:,1]\n",
    "\n",
    "del cat_clf,xgb_clf,lgb_clf,clf_test_pred,clf_train_pred\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressions With GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as catb\n",
    "tr_features = [_f for _f in df_train.columns if _f not in excluded_cols]\n",
    "\n",
    "#GroupKFold 交叉验证输出\n",
    "def get_out_fold(model,x_train,y_train,x_test):\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    NFOLDS = 10\n",
    "    \n",
    "    folds = get_folds(df=x_train, n_splits=NFOLDS)\n",
    "#     获取预测叶子节点Index\n",
    "#     train_index_features = np.zeros((ntrain,model.n_estimators))\n",
    "#     test_index_features = np.zeros((ntest,model.n_estimators))\n",
    "    \n",
    "    oof_train_pred = np.zeros((ntrain,))\n",
    "    oof_test_pred = np.zeros((ntest,))\n",
    "    oof_test_pred_skf = np.empty((NFOLDS, ntest))\n",
    "    #针对不同的模型采用不同的训练方式\n",
    "    for i, (dev_index, val_index) in enumerate(folds):\n",
    "        x_dev = x_train[tr_features].iloc[dev_index]\n",
    "        y_dev = y_train.iloc[dev_index]\n",
    "        x_val = x_train[tr_features].iloc[val_index]\n",
    "        y_val = y_train.iloc[val_index]\n",
    "\n",
    "        model.fit(x_dev, y_dev,eval_set=[(x_dev,y_dev),(x_val,y_val)],early_stopping_rounds=100,verbose=100)\n",
    "    \n",
    "        oof_test_pred_skf[i, :] = model.predict(x_test[tr_features],num_iteration=model.best_iteration_)\n",
    "        oof_train_pred[val_index] = model.predict(x_val,num_iteration=model.best_iteration_)\n",
    "#     train_index_features = model.predict(x_train[tr_features],pred_leaf=True)\n",
    "#     test_index_features = model.predict(x_test[tr_features],pred_leaf=True)\n",
    "    \n",
    "    oof_test_pred_skf[oof_test_pred_skf<0] = 0\n",
    "    oof_test_pred[:] = np.expm1(oof_test_pred_skf).mean(axis=0)\n",
    "    oof_train_pred = np.expm1(oof_train_pred)\n",
    "    \n",
    "    print(\"-\"*50+str(\"model training done！\")+\"-\"*50)\n",
    "    \n",
    "    return model,oof_test_pred.reshape(-1, 1),oof_train_pred.reshape(-1,1)\n",
    "\n",
    "lgb_params={\n",
    "    'learning_rate': 0.03,\n",
    "    'objective':'regression',\n",
    "    'n_estimators':5000,\n",
    "    'metric':'rmse',\n",
    "    'num_leaves': 31,\n",
    "    'verbose': 1,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"random_state\":42,\n",
    "    'max_depth': 15,\n",
    "    'lambda_l2': 0.02,\n",
    "    'lambda_l1': 0.004,\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id':1,\n",
    "    'gpu_device_id': 1,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.7,\n",
    "    'min_child_samples': 21\n",
    "}\n",
    "lgb_est = lgb.LGBMRegressor(**lgb_params)\n",
    "lgb_est,oof_lgb_test_pred,oof_lgb_train_pred = get_out_fold(lgb_est,df_train,train_y,df_test)\n",
    "# lgb_est.booster_.save_model('lgb_est_session.txt')\n",
    "\n",
    "# lgb_train_pred = pd.DataFrame({\"fullVisitorId\":train_df['fullVisitorId'].values,\"lgb_train_pred\":oof_lgb_train_pred.reshape(-1,)})\n",
    "# lgb_test_pred = pd.DataFrame({\"fullVisitorId\":test_df['fullVisitorId'].values,\"lgb_test_pred\":oof_lgb_test_pred.reshape(-1,)})\n",
    "# lgb_train_pred.to_csv('lgb_train_session_pred.csv',index=False)\n",
    "# lgb_test_pred.to_csv('lgb_test_session_pred.csv',index=False)\n",
    "\n",
    "#特征重要性分析\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "lgb.plot_importance(lgb_est,max_num_features=100, height=0.8, ax=ax)\n",
    "ax.grid(False)\n",
    "plt.title(\"LGBM - Feature Importance\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_out_fold2(model,x_train,y_train,x_test):\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    NFOLDS = 10\n",
    "    \n",
    "    folds = get_folds(df=x_train, n_splits=NFOLDS)\n",
    "#     获取预测叶子节点Index\n",
    "#     train_index_features = np.zeros((ntrain,model.n_estimators))\n",
    "#     test_index_features = np.zeros((ntest,model.n_estimators))\n",
    "    \n",
    "    oof_train_pred = np.zeros((ntrain,))\n",
    "    oof_test_pred = np.zeros((ntest,))\n",
    "    oof_test_pred_skf = np.empty((NFOLDS, ntest))\n",
    "    #针对不同的模型采用不同的训练方式\n",
    "    for i, (dev_index, val_index) in enumerate(folds):\n",
    "        x_dev = x_train[tr_features].iloc[dev_index]\n",
    "        y_dev = y_train.iloc[dev_index]\n",
    "        x_val = x_train[tr_features].iloc[val_index]\n",
    "        y_val = y_train.iloc[val_index]\n",
    "\n",
    "        model.fit(x_dev, y_dev,eval_set=[(x_dev,y_dev),(x_val,y_val)],early_stopping_rounds=100,verbose=100)\n",
    "    \n",
    "        oof_test_pred_skf[i, :] = model.predict(x_test[tr_features])\n",
    "        oof_train_pred[val_index] = model.predict(x_val)\n",
    "#     train_index_features = model.predict(x_train[tr_features],pred_leaf=True)\n",
    "#     test_index_features = model.predict(x_test[tr_features],pred_leaf=True)\n",
    "    \n",
    "    oof_test_pred_skf[oof_test_pred_skf<0] = 0\n",
    "    oof_test_pred[:] = np.expm1(oof_test_pred_skf).mean(axis=0)\n",
    "    oof_train_pred = np.expm1(oof_train_pred)\n",
    "    \n",
    "    print(\"-\"*50+str(\"model training done！\")+\"-\"*50)\n",
    "    \n",
    "    return model,oof_test_pred.reshape(-1, 1),oof_train_pred.reshape(-1,1)\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:linear',\n",
    "    'booster': 'gbtree',\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators':5000,\n",
    "    'max_depth': 22,\n",
    "    'min_child_weight': 57,\n",
    "    'gamma' : 1.45,\n",
    "    'alpha': 0.0,\n",
    "    'lambda': 0.0,\n",
    "    'subsample': 0.67,\n",
    "    'colsample_bytree': 0.054,\n",
    "    'colsample_bylevel': 0.50,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 20\n",
    "}\n",
    "xgb_est = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_est,oof_xgb_test_pred,oof_xgb_train_pred = get_out_fold2(xgb_est,df_train,train_y,df_test)\n",
    "# xgb_est.save_model('xgb_est_session.model')\n",
    "\n",
    "# xgb_train_pred = pd.DataFrame({\"fullVisitorId\":train_df['fullVisitorId'].values,\"xgb_train_pred\":oof_xgb_train_pred.reshape(-1,)})\n",
    "# xgb_test_pred = pd.DataFrame({\"fullVisitorId\":test_df['fullVisitorId'].values,\"xgb_test_pred\":oof_xgb_test_pred.reshape(-1,)})\n",
    "# xgb_train_pred.to_csv('xgb_train_session_pred.csv',index=False)\n",
    "# xgb_test_pred.to_csv('xgb_test_session_pred.csv',index=False)\n",
    "#特征重要性分析\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "xgb.plot_importance(xgb_est,max_num_features=100, height=0.8, ax=ax)\n",
    "ax.grid(False)\n",
    "plt.title(\"XGB - Feature Importance\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_params = {\n",
    "        'n_estimators':5000,\n",
    "        'learning_rate':0.01,\n",
    "        'max_depth':7,\n",
    "        'loss_function':'RMSE',\n",
    "        'eval_metric':'RMSE',\n",
    "        'random_state':42,\n",
    "        'bagging_temperature':0.2,\n",
    "        'od_type':'Iter',\n",
    "        'od_wait':20\n",
    "    }\n",
    "cat_est = catb.CatBoostRegressor(**cat_params)\n",
    "cat_est,oof_cat_test_pred,oof_cat_train_pred = get_out_fold2(cat_est,df_train,train_y,df_test)\n",
    "\n",
    "# cat_est.save_model('catb_est_session.mlmodel',format=\"coreml\")\n",
    "# cat_train_pred = pd.DataFrame({\"fullVisitorId\":train_df['fullVisitorId'].values,\"cat_train_pred\":oof_cat_train_pred.reshape(-1,)})\n",
    "# cat_test_pred = pd.DataFrame({\"fullVisitorId\":test_df['fullVisitorId'].values,\"cat_test_pred\":oof_cat_test_pred.reshape(-1,)})\n",
    "# cat_train_pred.to_csv('cat_train_session_pred.csv',index=False)\n",
    "# cat_test_pred.to_csv('cat_test_session_pred.csv',index=False)\n",
    "\n",
    "cat_est.get_feature_importance(prettified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oof_test_pred = oof_lgb_test_pred*0.2+ oof_xgb_test_pred*0.4 + oof_cat_test_pred*0.4\n",
    "oof_train_pred = oof_lgb_train_pred*0.2+ oof_xgb_train_pred*0.4  + oof_cat_train_pred*0.4 \n",
    "del oof_lgb_test_pred,oof_xgb_test_pred,oof_cat_test_pred\n",
    "del oof_lgb_train_pred,oof_xgb_train_pred,oof_cat_train_pred\n",
    "del lgb_est,xgb_est,cat_est\n",
    "gc.collect()\n",
    "\n",
    "df_train.to_csv('./features/'+'df_trainV2.csv',index=None)\n",
    "df_test.to_csv('./features/'+'df_testV2.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建基于用户的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./features/'+'df_trainV2.csv',dtype={'fullVisitorId':np.str,'date':np.str},low_memory=False)\n",
    "df_test = pd.read_csv('./features/'+'df_testV2.csv',dtype={'fullVisitorId':np.str,'date':np.str},low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将预测收入作为特征加入到数据集中\n",
    "df_train['X_pred'] = oof_train_pred\n",
    "df_test['X_pred'] = oof_test_pred\n",
    "\n",
    "df_test = df_test[df_test.date>='20180815']\n",
    "\n",
    "tr_features = [_f for _f in df_train.columns if _f not in excluded_cols]\n",
    "\n",
    "train_data = df_train[tr_features+['fullVisitorId']].groupby('fullVisitorId').mean().reset_index()\n",
    "test_data = df_test[tr_features+['fullVisitorId']].groupby('fullVisitorId').mean().reset_index()\n",
    "\n",
    "train_pred_list = df_train[['fullVisitorId', 'X_pred']].groupby('fullVisitorId')\\\n",
    "    .apply(lambda df: list(df.X_pred))\\\n",
    "    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})\n",
    "test_pred_list = df_test[['fullVisitorId','X_pred']].groupby('fullVisitorId')\\\n",
    "    .apply(lambda df:list(df.X_pred))\\\n",
    "    .apply(lambda x:{'pred_'+str(i):pred for i,pred in enumerate(x)})\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "train_all_preds = pd.DataFrame(list(train_pred_list.values),index = train_data['fullVisitorId'])\n",
    "test_all_preds = pd.DataFrame(list(test_pred_list.values),index = test_data['fullVisitorId'])\n",
    "\n",
    "print(train_all_preds.shape,test_all_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将test_all_preds中缺失的列补齐\n",
    "for col in train_all_preds.columns:\n",
    "    if col not in test_all_preds.columns:\n",
    "        test_all_preds[col] = np.nan\n",
    "print(train_all_preds.shape,test_all_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对预测的收入进行特征工程\n",
    "train_all_preds['pred_max'] = np.log1p(train_all_preds.mean(axis=1))\n",
    "train_all_preds['pred_min'] = np.log1p(train_all_preds.mean(axis=1))\n",
    "train_all_preds['pred_sum'] = np.log1p(train_all_preds.fillna(0).sum(axis=1))\n",
    "train_all_preds['pred_median'] = np.log1p(train_all_preds.median(axis=1))\n",
    "train_all_preds['pred_nullrate'] = train_all_preds.isnull().sum(axis=1)/len(train_all_preds.columns)\n",
    "\n",
    "test_all_preds['pred_max'] = np.log1p(test_all_preds.mean(axis=1))\n",
    "test_all_preds['pred_min'] = np.log1p(test_all_preds.mean(axis=1))\n",
    "test_all_preds['pred_median'] = np.log1p(test_all_preds.median(axis=1))\n",
    "test_all_preds['pred_sum'] = np.log1p(test_all_preds.fillna(0).sum(axis=1))\n",
    "test_all_preds['pred_nullrate'] = test_all_preds.isnull().sum(axis=1)/len(test_all_preds.columns)\n",
    "\n",
    "train_data = train_data.merge(train_all_preds.reset_index(),on='fullVisitorId',how='left')\n",
    "test_data = test_data.merge(test_all_preds.reset_index(),on='fullVisitorId',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_train_y = df_train[['fullVisitorId','totals.transactionRevenue']].groupby('fullVisitorId').sum().reset_index()\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(user_train_y.shape)\n",
    "\n",
    "del df_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def modelXGBAndLGB(trn_data,te_data,trnUsery):\n",
    "    \n",
    "    folds = get_folds(df=trn_data[['fullVisitorId','totals.pageviews']], n_splits=5)\n",
    "    \n",
    "    oof_preds = np.zeros(trn_data.shape[0])\n",
    "    sub_preds = np.zeros(te_data.shape[0])\n",
    "    vis_importances = pd.DataFrame()\n",
    "    trn_features = [f for f in trn_data.columns if f!='fullVisitorId']\n",
    "    params={'learning_rate': 0.03,\n",
    "        'n_estimators':2000,\n",
    "        'objective':'regression',\n",
    "        'metric':'rmse',\n",
    "        'num_leaves': 32,\n",
    "        'verbose': 1,\n",
    "        \"subsample\": 0.99,\n",
    "        \"colsample_bytree\": 0.99,\n",
    "        \"random_state\":42,\n",
    "        'max_depth': 15,\n",
    "        'lambda_l2': 0.02,\n",
    "        'lambda_l1': 0.004,\n",
    "        'min_child_samples': 21\n",
    "       }\n",
    "    xgb_params = {\n",
    "            'objective': 'reg:linear',\n",
    "            'booster': 'gbtree',\n",
    "            'learning_rate': 0.02,\n",
    "            'n_estimators':2000,\n",
    "            'max_depth': 22,\n",
    "            'min_child_weight': 57,\n",
    "            'gamma' : 1.45,\n",
    "            'alpha': 0.0,\n",
    "            'lambda': 0.0,\n",
    "            'subsample': 0.67,\n",
    "            'colsample_bytree': 0.054,\n",
    "            'colsample_bylevel': 0.50,\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 456\n",
    "        }\n",
    "    xg = xgb.XGBRegressor(**xgb_params)\n",
    "    reg = lgb.LGBMRegressor(**params)\n",
    "\n",
    "    for fold_, (trn_, val_) in enumerate(folds):\n",
    "        trn_x, trn_y = trn_data[trn_features].iloc[trn_], trnUsery['totals.transactionRevenue'].iloc[trn_]\n",
    "        val_x, val_y = trn_data[trn_features].iloc[val_], trnUsery['totals.transactionRevenue'].iloc[val_]\n",
    "\n",
    "        print('-'*40+str(\"XGBoost is training\"+'-'*40))\n",
    "        xg.fit(\n",
    "            trn_x, np.log1p(trn_y),\n",
    "            eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n",
    "            early_stopping_rounds=100,\n",
    "            eval_metric='rmse',\n",
    "            verbose=100\n",
    "        )\n",
    "        print('-'*40+str(\"LGBM is training\"+'-'*40))\n",
    "        reg.fit(\n",
    "            trn_x, np.log1p(trn_y),\n",
    "            eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n",
    "            eval_names=['TRAIN', 'VALID'],\n",
    "            early_stopping_rounds=100,\n",
    "            eval_metric='rmse',\n",
    "            verbose=100\n",
    "        )\n",
    "\n",
    "        imp_df = pd.DataFrame()\n",
    "        imp_df['feature'] = trn_x.columns\n",
    "        imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n",
    "\n",
    "        imp_df['fold'] = fold_ + 1\n",
    "        vis_importances = pd.concat([vis_importances, imp_df], axis=0, sort=False)\n",
    "\n",
    "        oof_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n",
    "        oof_preds[oof_preds < 0] = 0\n",
    "\n",
    "        # Make sure features are in the same order\n",
    "        _preds = reg.predict(te_data[trn_features], num_iteration=reg.best_iteration_)\n",
    "        _preds[_preds < 0] = 0\n",
    "\n",
    "        pre = xg.predict(te_data[trn_features])\n",
    "        pre[pre<0]=0\n",
    "\n",
    "        sub_preds += (_preds / len(folds)) * 0.5 + (pre / len(folds)) * 0.5\n",
    "        \n",
    "    mean_squared_error(np.log1p(trnUsery['totals.transactionRevenue']), oof_preds) ** .5\n",
    "        \n",
    "    return reg,xg,sub_preds,vis_importances\n",
    "\n",
    "lgb_est,xgb_est,sub_preds,vis_importances = modelXGBAndLGB(train_data,test_data,user_train_y)\n",
    "\n",
    "# lgb_est.booster_.save_model('lgb_est_user.txt')\n",
    "# xgb_est.save_model('xgb_est_user.model')\n",
    "# del lgb_est,xgb_est\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "flagtime  = datetime.datetime.now()\n",
    "flagtime = datetime.datetime.strftime(flagtime,'%Y%m%d%H%M')\n",
    "\n",
    "vis_importances['gain_log'] = np.log1p(vis_importances['gain'])\n",
    "mean_gain = vis_importances[['gain', 'feature']].groupby('feature').mean()\n",
    "vis_importances['mean_gain'] = vis_importances['feature'].map(mean_gain['gain'])\n",
    "\n",
    "plt.figure(figsize=(8, 25))\n",
    "sns.barplot(x='gain_log', y='feature', data=vis_importances.sort_values('mean_gain', ascending=False).iloc[:300])\n",
    "\n",
    "test_data['PredictedLogRevenue'] = sub_preds\n",
    "# test_data[['fullVisitorId','PredictedLogRevenue']].to_csv(submission_dir+'submission_tuning2_'+flagtime+'.csv', index=False)\n",
    "df_sub = pd.DataFrame()\n",
    "df_sub['fullVisitorId'] = list(set(list(df_test['fullVisitorId'])))\n",
    "df_sub = df_sub.merge(test_data[['fullVisitorId','PredictedLogRevenue']],on=['fullVisitorId'],how='left')\n",
    "df_sub.fillna(0,inplace=True)\n",
    "df_sub.to_csv(submission_dir+'submission_tuning2_'+flagtime+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
