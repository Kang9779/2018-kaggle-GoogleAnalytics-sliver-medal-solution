{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from load_df import *\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from pandas.io.json import json_normalize\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from plotly import tools\n",
    "\n",
    "py.init_notebook_mode(connected=True)\n",
    "color = sns.color_palette()\n",
    "feat_orignal_dir = \"features_orignal/\"\n",
    "submission_dir = 'submission/'\n",
    "os.environ['CUDA_DEVICE_ORDER']=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# 加载原始数据\n",
    "train_path = feat_orignal_dir + 'train.csv'\n",
    "test_path = feat_orignal_dir + 'test.csv'\n",
    "\n",
    "df_train,df_test = get_df(train_path,test_path)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_const_cols(train_df,test_df):\n",
    "    #查询出具有列元素全部相同的列名\n",
    "    const_cols = [c for c in tqdm_notebook(train_df.columns) if train_df[c].nunique(dropna=False)==1 and c!='totals.visits']\n",
    "    if len(const_cols):\n",
    "        cols_to_drop = const_cols + [\"sessionId\"]+['trafficSource.adwordsClickInfo.page']\n",
    "        train_df = train_df.drop(cols_to_drop+['trafficSource.campaignCode'],axis=1)\n",
    "        test_df = test_df.drop(cols_to_drop,axis=1)\n",
    "    return train_df,test_df\n",
    "df_train,df_test = remove_const_cols(df_train,df_test)\n",
    "df_train.sort_values(['date'],ascending=True,inplace=True)\n",
    "df_train['totals.transactionRevenue'] = df_train['totals.transactionRevenue'].apply(lambda x:np.log1p(float(x)) if float(x) > 0 else 0)\n",
    "df_test.sort_values(['date'],ascending=True,inplace=True)\n",
    "df_test['is_test'] = True\n",
    "\n",
    "df_data = pd.concat((df_train,df_test))\n",
    "\n",
    "print(df_train.shape,df_test.shape)\n",
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories Features && Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#类别特征处理\n",
    "device_browers = list(df_data['device.browser'].value_counts().reset_index()['index'][0:30])\n",
    "device_os = list(df_data['device.operatingSystem'].value_counts().reset_index()['index'][0:15])\n",
    "\n",
    "geoNetwork_cities = list(df_data['geoNetwork.city'].value_counts().reset_index()['index'][0:10])\n",
    "geoNetwork_country = list(df_data['geoNetwork.country'].value_counts().reset_index()['index'][0:20])\n",
    "geoNetwork_metro = list(df_data['geoNetwork.metro'].value_counts().reset_index()['index'][0:40])\n",
    "geoNetwork_networkDomain = list(df_data['geoNetwork.networkDomain'].value_counts().reset_index()['index'][0:40])\n",
    "\n",
    "def browser_mapping(x):\n",
    "    if x in device_browers:\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return 'others'\n",
    "\n",
    "def geoNetwork_city(x):\n",
    "    if x in geoNetwork_cities:\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return 'others'\n",
    "    \n",
    "def geoNetwork_countries(x):\n",
    "    if x in geoNetwork_country:\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return 'others'\n",
    "\n",
    "def adcontents_mapping(x):\n",
    "    if  ('google' in x):\n",
    "        return 'google'\n",
    "    elif  ('placement' in x) | ('placememnt' in x):\n",
    "        return 'placement'\n",
    "    elif '(not set)' in x or 'nan' in x:\n",
    "        return x\n",
    "    elif 'ad' in x:\n",
    "        return 'ad'\n",
    "    else:\n",
    "        return 'others'\n",
    "    \n",
    "def device_operatingSystem(x):\n",
    "    if x in device_os:\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return 'others'\n",
    "\n",
    "def traficSource_referralPath(x):\n",
    "    if x == '/':\n",
    "        return '/'\n",
    "    elif 'yt/about/' in x:\n",
    "        return x\n",
    "    elif 'google' in x:\n",
    "        return '/google'\n",
    "    elif 'mail' in x:\n",
    "        return '/mail'\n",
    "    elif 'yt/advertise/' in x:\n",
    "        return x\n",
    "    elif 'offer/2145' in x:\n",
    "        return x\n",
    "    elif 'yt/creators/' in x:\n",
    "        return x\n",
    "    elif 'pagead/ads' in x:\n",
    "        return x\n",
    "    elif '/intl/' in x:\n",
    "        return x;\n",
    "    elif 'shirt' in x:\n",
    "        return x;\n",
    "    elif '/analytics/app/' in x:\n",
    "        return x\n",
    "    elif 'using-the-logo':\n",
    "        return x\n",
    "    elif '/moma' in x:\n",
    "        return x\n",
    "    else:\n",
    "        return '/others'  \n",
    "    \n",
    "def source_mapping(x):\n",
    "    if  ('google' in x):\n",
    "        return 'google'\n",
    "    elif('(direct)'in x):\n",
    "        return x;\n",
    "    elif  ('youtube' in x):\n",
    "        return 'youtube'\n",
    "    elif '(not set)' in x or 'nan' in x:\n",
    "        return x\n",
    "    elif 'yahoo' in x:\n",
    "        return 'yahoo'\n",
    "    elif 'facebook' in x:\n",
    "        return 'facebook'\n",
    "    elif 'reddit' in x:\n",
    "        return 'reddit'\n",
    "    elif 'bing' in x:\n",
    "        return 'bing'\n",
    "    elif 'quora' in x:\n",
    "        return 'quora'\n",
    "    elif 'outlook' in x:\n",
    "        return 'outlook'\n",
    "    elif 'linkedin' in x:\n",
    "        return 'linkedin'\n",
    "    elif 'pinterest' in x:\n",
    "        return 'pinterest'\n",
    "    elif 'ask' in x:\n",
    "        return 'ask'\n",
    "    elif 'siliconvalley' in x:\n",
    "        return 'siliconvalley'\n",
    "    elif 'lunametrics' in x:\n",
    "        return 'lunametrics'\n",
    "    elif 'amazon' in x:\n",
    "        return 'amazon'\n",
    "    elif 'mysearch' in x:\n",
    "        return 'mysearch'\n",
    "    elif 'qiita' in x:\n",
    "        return 'qiita'\n",
    "    elif 'messenger' in x:\n",
    "        return 'messenger'\n",
    "    elif 'twitter' in x:\n",
    "        return 'twitter'\n",
    "    elif 't.co' in x:\n",
    "        return 't.co'\n",
    "    elif 'vk.com' in x:\n",
    "        return 'vk.com'\n",
    "    elif 'search' in x:\n",
    "        return 'search'\n",
    "    elif 'edu' in x:\n",
    "        return 'edu'\n",
    "    elif 'mail' in x:\n",
    "        return 'mail'\n",
    "    elif 'ad' in x:\n",
    "        return 'ad'\n",
    "    elif 'golang' in x:\n",
    "        return 'golang'\n",
    "    elif 'direct' in x:\n",
    "        return 'direct'\n",
    "    elif 'dealspotr' in x:\n",
    "        return 'dealspotr'\n",
    "    elif 'sashihara' in x:\n",
    "        return 'sashihara'\n",
    "    elif 'phandroid' in x:\n",
    "        return 'phandroid'\n",
    "    elif 'baidu' in x:\n",
    "        return 'baidu'\n",
    "    elif 'mdn' in x:\n",
    "        return 'mdn'\n",
    "    elif 'duckduckgo' in x:\n",
    "        return 'duckduckgo'\n",
    "    elif 'seroundtable' in x:\n",
    "        return 'seroundtable'\n",
    "    elif 'metrics' in x:\n",
    "        return 'metrics'\n",
    "    elif 'sogou' in x:\n",
    "        return 'sogou'\n",
    "    elif 'businessinsider' in x:\n",
    "        return 'businessinsider'\n",
    "    elif 'github' in x:\n",
    "        return 'github'\n",
    "    elif 'gophergala' in x:\n",
    "        return 'gophergala'\n",
    "    elif 'yandex' in x:\n",
    "        return 'yandex'\n",
    "    elif 'msn' in x:\n",
    "        return 'msn'\n",
    "    elif 'dfa' in x:\n",
    "        return 'dfa'\n",
    "    elif 'feedly' in x:\n",
    "        return 'feedly'\n",
    "    elif 'arstechnica' in x:\n",
    "        return 'arstechnica'\n",
    "    elif 'squishable' in x:\n",
    "        return 'squishable'\n",
    "    elif 'flipboard' in x:\n",
    "        return 'flipboard'\n",
    "    elif 't-online.de' in x:\n",
    "        return 't-online.de'\n",
    "    elif 'sm.cn' in x:\n",
    "        return 'sm.cn'\n",
    "    elif 'wow' in x:\n",
    "        return 'wow'\n",
    "    elif 'baidu' in x:\n",
    "        return 'baidu'\n",
    "    elif 'partners' in x:\n",
    "        return 'partners'\n",
    "    else:\n",
    "        return 'others'\n",
    "\n",
    "for df in [df_data]:  \n",
    "    df['device.browser'] = df['device.browser'].map(lambda x:browser_mapping(str(x))).astype('str')\n",
    "    df['device.operatingSystem'] = df['device.operatingSystem'].map(lambda x:device_operatingSystem(str(x))).astype('str')\n",
    "    df['trafficSource.adContent'] = df['trafficSource.adContent'].map(lambda x:adcontents_mapping(str(x).lower())).astype('str')\n",
    "    df['trafficSource.source'] = df['trafficSource.source'].map(lambda x:source_mapping(str(x).lower())).astype('str')\n",
    "    df['geoNetwork.city'] = df['geoNetwork.city'].map(lambda x:geoNetwork_city(str(x))).astype('str')\n",
    "    df['geoNetwork.country'] = df['geoNetwork.country'].map(lambda x:geoNetwork_countries(str(x))).astype('str')\n",
    "    df['trafficSource.referralPath'] = df['trafficSource.referralPath'].map(lambda x:traficSource_referralPath(str(x))).astype('str')\n",
    "\n",
    "#将具有从属关系的特征进行合并:地区从属关系，市场分区\n",
    "for df in [df_data]:\n",
    "    print(\"... process device ...\")\n",
    "    df['source.country'] = df['trafficSource.source'] + '_' + df['geoNetwork.country']\n",
    "    df['campaign.medium'] = df['trafficSource.campaign'] + '_' + df['trafficSource.medium']\n",
    "    df['browser.category'] = df['device.browser'] + '_' + df['device.deviceCategory']\n",
    "    df['browser.os'] = df['device.browser'] + '_' + df['device.operatingSystem']\n",
    "\n",
    "#将设备中具有从属关系的特征整合\n",
    "def custom(df):\n",
    "    print('... custom ...')\n",
    "    df['device_deviceCategory_channelGrouping'] = df['device.deviceCategory'] + \"_\" + df['channelGrouping']\n",
    "    df['channelGrouping_browser'] = df['device.browser'] + \"_\" + df['channelGrouping']\n",
    "    df['channelGrouping_OS'] = df['device.operatingSystem'] + \"_\" + df['channelGrouping']\n",
    "    \n",
    "    df['city_continent_country_metro_networkDomain_region_subContinent'] = df['geoNetwork.city']+\"_\"+df['geoNetwork.continent']+\"_\"+df['geoNetwork.country']+\"_\"+df['geoNetwork.metro']+\"_\"+df['geoNetwork.networkDomain']+\"_\"+df['geoNetwork.region']+df['geoNetwork.subContinent']\n",
    "    \n",
    "    for i in ['geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country','geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region','geoNetwork.subContinent']:\n",
    "        for j in ['device.browser','device.deviceCategory', 'device.operatingSystem', 'trafficSource.source']:\n",
    "            df[i + \"_\" + j] = df[i] + \"_\" + df[j]\n",
    "    \n",
    "    df['content.source'] = df['trafficSource.adContent'] + \"_\" + df['source.country']\n",
    "    df['medium.source'] = df['trafficSource.medium'] + \"_\" + df['source.country']\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_data = custom(df_data)\n",
    "\n",
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "excluded_cols = ['date','fullVisitorId','totals.transactionRevenue','visitId','visitStartTime','is_test']\n",
    "cat_cols = [col for col in df_data.columns if df_data[col].dtype ==\"object\" and col not in excluded_cols]\n",
    "\n",
    "# label encode the categorical variables and convert the numerical variables to float\n",
    "def label_encoding(df):\n",
    "    for col in tqdm_notebook(cat_cols):\n",
    "        if df[col].dtype == bool:#将bool变量转成整数\n",
    "            df[col] = train_df[col].astype(int)\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(df[col].values.astype('str')))\n",
    "        \n",
    "        df[col] = lbl.transform(list(df[col].values.astype('str')))\n",
    "        \n",
    "    return df\n",
    "\n",
    "df_data = label_encoding(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downCast_dtype(df):\n",
    "    '''\n",
    "        对数据类型进行转换\n",
    "    '''\n",
    "    float_cols = [c for c in df if df[c].dtype == 'float64']\n",
    "    int_cols = [c for c in df if df[c].dtype == 'int64']\n",
    "    \n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df\n",
    "df_data = downCast_dtype(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from datetime import date,timedelta\n",
    "\n",
    "# holiday = ['01-01','02-01','02-12','02-14','02-22','03-07','03-17','04-01','05-01','05-14','06-01','06-21','06-14','07-01','08-01','09-01','10-01','10-31','11-01','12-24','12-25']\n",
    "\n",
    "# df_data['visitStartTime'] = pd.to_datetime(df_data['visitStartTime'],unit='s')\n",
    "# df_data['month'] = df_data['visitStartTime'].dt.month\n",
    "# df_data['week'] = df_data['visitStartTime'].dt.week\n",
    "# df_data['day'] = df_data['visitStartTime'].dt.day\n",
    "# df_data['hour'] = df_data['visitStartTime'].dt.hour\n",
    "# df_data[\"week_day\"] = df_data['visitStartTime'].dt.weekday\n",
    "# for h in holiday:\n",
    "#     h_month,h_day = h.split('-')\n",
    "#     df_data['is_holiday'] = ((df_data['month']==int(h_month))&(df_data['day']==int(h_day))|(df_data['week_day']==5)|(df_data['week_day']==6))*1\n",
    "\n",
    "# #某日前两天、和后两天是否为假期\n",
    "# df_data['prev_day_is_holiday'] = df_data['is_holiday'].shift().fillna(0)\n",
    "# df_data['pre2_day_is_holiday'] = df_data['is_holiday'].shift(2).fillna(0)\n",
    "# df_data['next_day_is_holiday'] = df_data['is_holiday'].shift(-1).fillna(0)\n",
    "# df_data['next2_day_is_holiday'] = df_data['is_holiday'].shift(-2).fillna(0)\n",
    "\n",
    "# df_data['month.unique.user.count'] = df_data.groupby('month')['fullVisitorId'].transform('nunique')\n",
    "# df_data['day.unique.user.count'] = df_data.groupby('day')['fullVisitorId'].transform('nunique')\n",
    "# df_data['weekday.unique.user.count'] = df_data.groupby('week_day')['fullVisitorId'].transform('nunique')\n",
    "\n",
    "# df_data['next_session_1'] = (\n",
    "#     df_data['visitStartTime'] - df_data[['fullVisitorId', 'visitStartTime']].groupby('fullVisitorId')['visitStartTime'].shift(1)\n",
    "# ).astype(np.int64) // 1e9 // 60 // 60\n",
    "# df_data['next_session_2'] = (\n",
    "#     df_data['visitStartTime'] - df_data[['fullVisitorId', 'visitStartTime']].groupby('fullVisitorId')['visitStartTime'].shift(-1)\n",
    "# ).astype(np.int64) // 1e9 // 60 // 60\n",
    "\n",
    "# df_data.drop(columns=['visitStarTime'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date,timedelta\n",
    "from funcUtils import *\n",
    "#获得核函数 PrEp\n",
    "PrOriginalEp = np.zeros((2000,2000))\n",
    "PrOriginalEp[1,0] = 1\n",
    "PrOriginalEp[2,range(2)] = [0.5,0.5]\n",
    "for i in range(3,2000):\n",
    "    scale = (i-1)/2.\n",
    "    x = np.arange(-(i+1)/2.+1, (i+1)/2., step=1)/scale\n",
    "    y = 3./4.*(1-x**2)\n",
    "    y = y/np.sum(y)\n",
    "    PrOriginalEp[i, range(i)] = y\n",
    "PrEp = PrOriginalEp.copy()\n",
    "for i in range(3, 2000):\n",
    "    PrEp[i,:i] = (PrEp[i,:i]*i+1)/(i+1)\n",
    "\n",
    "def dateGap(x,y):\n",
    "    year = x[:4]\n",
    "    month = x[4:6]\n",
    "    day = x[6:]\n",
    "    return (date(int(year),int(month),int(day))-y).days\n",
    "\n",
    "def datePrepro(df):\n",
    "    holiday = ['01-01','02-01','02-12','02-14','02-22','03-07','03-17','04-01','05-01','05-14','06-01','06-21','06-14','07-01','08-01','09-01','10-01','10-31','11-01','12-24','12-25']\n",
    "\n",
    "    df['visitStartTime'] = pd.to_datetime(df['visitStartTime'],unit='s')\n",
    "    df['month'] = df['visitStartTime'].dt.month\n",
    "    df['week'] = df['visitStartTime'].dt.week\n",
    "    df['day'] = df['visitStartTime'].dt.day\n",
    "    df['hour'] = df['visitStartTime'].dt.hour\n",
    "    df[\"week_day\"] = df['visitStartTime'].dt.weekday\n",
    "    for h in holiday:\n",
    "        h_month,h_day = h.split('-')\n",
    "        df['is_holiday'] = ((df['month']==int(h_month))&(df['day']==int(h_day))|(df['week_day']==5)|(df['week_day']==6))*1\n",
    "\n",
    "    #某日前两天、和后两天是否为假期\n",
    "    df['prev_day_is_holiday'] = df['is_holiday'].shift().fillna(0)\n",
    "    df['pre2_day_is_holiday'] = df['is_holiday'].shift(2).fillna(0)\n",
    "    df['next_day_is_holiday'] = df['is_holiday'].shift(-1).fillna(0)\n",
    "    df['next2_day_is_holiday'] = df['is_holiday'].shift(-2).fillna(0)\n",
    "\n",
    "    df['month.unique.user.count'] = df.groupby('month')['fullVisitorId'].transform('nunique')\n",
    "    df['day.unique.user.count'] = df.groupby('day')['fullVisitorId'].transform('nunique')\n",
    "    df['weekday.unique.user.count'] = df.groupby('week_day')['fullVisitorId'].transform('nunique')\n",
    "\n",
    "    df['next_session_1'] = (\n",
    "        df['visitStartTime'] - df[['fullVisitorId', 'visitStartTime']].groupby('fullVisitorId')['visitStartTime'].shift(1)\n",
    "    ).astype(np.int64) // 1e9 // 60 // 60\n",
    "    df['next_session_2'] = (\n",
    "        df['visitStartTime'] - df[['fullVisitorId', 'visitStartTime']].groupby('fullVisitorId')['visitStartTime'].shift(-1)\n",
    "    ).astype(np.int64) // 1e9 // 60 // 60\n",
    "\n",
    "#     df_date = pd.to_datetime(df['date'])\n",
    "    days_of_months = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    #当天距离月初月末的距离\n",
    "    df[\"days_to_side\"] = df['visitStartTime'].apply(\n",
    "                            lambda x: min(x.day, days_of_months[x.month-1]-x.day))\n",
    "    df['day'] = df['day'].apply(lambda x:0 if x<=7 else 2 if x>=24 else 1)\n",
    "    df.drop(columns=['visitStartTime'],axis=1,inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def getFeatures(df_label,df_train):\n",
    "    \n",
    "    df_train = datePrepro(df_train)\n",
    "    df_label = datePrepro(df_label)\n",
    "    \n",
    "    df_label = feat_kernelMedian(df_label,df_train,['fullVisitorId','day','week_day'],\n",
    "                                                             'totals.hits',PrEp,'hits_weekday')\n",
    "    df_label = feat_kernelMedian(df_label,df_train,['fullVisitorId','day','week_day'],\n",
    "                                                             'totals.visits',PrEp,'visits_weekday')\n",
    "    df_label = feat_kernelMedian(df_label,df_train,['fullVisitorId','day','week_day'],\n",
    "                                                             'totals.pageviews',PrEp,'pageviews_weekday')\n",
    "    df_label = feat_kernelMedian(df_label,df_train,['fullVisitorId','day','week_day'],\n",
    "                                                             'visitNumber',PrEp,'visitNumber_weekday')\n",
    "    #需要进行统计的量\n",
    "    targetCols = ['totals.hits','totals.visits','totals.pageviews','visitNumber','totals.transactionRevenue']\n",
    "    \n",
    "    for i in [21,35,63,140,280,365]:\n",
    "#     for i in [21]:\n",
    "        df_select = df_train[df_train.day_gap>=-i].copy()\n",
    "        for col in targetCols: \n",
    "            print(col)\n",
    "            df_label = feat_median(df_label,df_select,['fullVisitorId'],col,'%s_median_%s'%(col,i))\n",
    "            df_label = feat_mean(df_label,df_select,['fullVisitorId'],col,'%s_mean_%s'%(col,i))\n",
    "            df_label = feat_kernelMedian(df_label,df_select,['fullVisitorId'],col,PrEp,'%s_kernelMed_%s'%(col,i))\n",
    "            df_label = feat_max(df_label,df_select,['fullVisitorId'],col,'%s_max_%i'%(col,i))\n",
    "            df_label = feat_min(df_label,df_select,['fullVisitorId'],col,'%s_min_%i'%(col,i))\n",
    "            df_label = feat_std(df_label,df_select,['fullVisitorId'],col,'%s_std_%i'%(col,i))\n",
    "            df_label = feat_count(df_label,df_select,['fullVisitorId'],col,'%s_count_%i'%(col,i))\n",
    "            \n",
    "            df_label = feat_kernelMedian(df_label,df_select,['fullVisitorId','month'],col,PrEp,'%s_month_kernelMed_%s'%(col,i))\n",
    "            df_label = feat_mean(df_label,df_select,['fullVisitorId','month'],col,'%s_month_mean_%s'%(col,i))\n",
    "            df_label = feat_max(df_label,df_select,['fullVisitorId','month'],col,'%s_month_max_%i'%(col,i))\n",
    "            df_label = feat_min(df_label,df_select,['fullVisitorId','month'],col,'%s_month_min_%i'%(col,i))\n",
    "            df_label = feat_std(df_label,df_select,['fullVisitorId','month'],col,'%s_month_std_%i'%(col,i))\n",
    "            df_label = feat_count(df_label,df_select,['fullVisitorId','month'],col,'%s_month_count_%i'%(col,i))\n",
    "            \n",
    "            df_label = feat_kernelMedian(df_label,df_select,['fullVisitorId','week_day'],col,PrEp,'%s_week_kernelMed_%s'%(col,i))\n",
    "            df_label = feat_mean(df_label,df_select,['fullVisitorId','week_day'],col,'%s_week_mean_%s'%(col,i))\n",
    "            df_label = feat_max(df_label,df_select,['fullVisitorId','week_day'],col,'%s_week_max_%i'%(col,i))\n",
    "            df_label = feat_min(df_label,df_select,['fullVisitorId','week_day'],col,'%s_week_min_%i'%(col,i))\n",
    "            df_label = feat_std(df_label,df_select,['fullVisitorId','week_day'],col,'%s_week_std_%i'%(col,i))\n",
    "            df_label = feat_count(df_label,df_select,['fullVisitorId','week_day'],col,'%s_week_count_%i'%(col,i))\n",
    "            \n",
    "            #节假日\n",
    "            df_label = feat_kernelMedian(df_label,df_select,['fullVisitorId','is_holiday'],col,PrEp,'%s_holiday_kernelMed_%s'%(col,i))\n",
    "            df_label = feat_mean(df_label,df_select,['fullVisitorId','is_holiday'],col,'%s_holiday_mean_%s'%(col,i))\n",
    "            df_label = feat_max(df_label,df_select,['fullVisitorId','is_holiday'],col,'%s_holiday_max_%i'%(col,i))\n",
    "            df_label = feat_min(df_label,df_select,['fullVisitorId','is_holiday'],col,'%s_holiday_min_%i'%(col,i))\n",
    "            df_label = feat_std(df_label,df_select,['fullVisitorId','is_holiday'],col,'%s_holiday_std_%i'%(col,i))\n",
    "            df_label = feat_count(df_label,df_select,['fullVisitorId','is_holiday'],col,'%s_holiday_count_%i'%(col,i))\n",
    "        \n",
    "        df_label['hits/pageviews_median_%i'%i] = df_label['totals.hits_median_%i'%i].values/df_label['totals.pageviews_median_%i'%i].values\n",
    "        df_label['hits/pageviews_mean_%i'%i] = df_label['totals.hits_mean_%i'%i].values/df_label['totals.pageviews_mean_%i'%i].values\n",
    "        df_label['hits/pageviews_max_%i'%i] = df_label['totals.hits_max_%i'%i].values/df_label['totals.pageviews_max_%i'%i].values\n",
    "        df_label['hits/pageviews_kernelMed_%i'%i] = df_label['totals.hits_kernelMed_%i'%i].values/df_label['totals.pageviews_kernelMed_%i'%i].values\n",
    "        \n",
    "#         df_label['hits/pageviews_month_median_%'%i] = df_label['totals.hits_month_median_%'%i].values/df_label['totals.pageviews_month_median_%'%i].values\n",
    "        df_label['hits/pageviews_month_mean_%i'%i] = df_label['totals.hits_month_mean_%i'%i].values/df_label['totals.pageviews_month_mean_%i'%i].values\n",
    "        df_label['hits/pageviews_month_max_%i'%i] = df_label['totals.hits_month_max_%i'%i].values/df_label['totals.pageviews_month_max_%i'%i].values\n",
    "        df_label['hits/pageviews_month_kernelMed_%i'%i] = df_label['totals.hits_month_kernelMed_%i'%i].values/df_label['totals.pageviews_month_kernelMed_%i'%i].values\n",
    "        \n",
    "#         df_label['hits/pageviews_week_median_%'%i] = df_label['totals.hits_week_median_%'%i].values/df_label['totals.pageviews_week_median_%'%i].values\n",
    "        df_label['hits/pageviews_weel_mean_%i'%i] = df_label['totals.hits_week_mean_%i'%i].values/df_label['totals.pageviews_week_mean_%i'%i].values\n",
    "        df_label['hits/pageviews_week_max_%i'%i] = df_label['totals.hits_week_max_%i'%i].values/df_label['totals.pageviews_week_max_%i'%i].values\n",
    "        df_label['hits/pageviews_week_kernelMed_%i'%i] = df_label['totals.hits_week_kernelMed_%i'%i].values/df_label['totals.pageviews_week_kernelMed_%i'%i].values\n",
    "        \n",
    "        df_label['hits/pageviews_holiday_mean_%i'%i] = df_label['totals.hits_holiday_mean_%i'%i].values/df_label['totals.pageviews_holiday_mean_%i'%i].values\n",
    "        df_label['hits/pageviews_holiday_max_%i'%i] = df_label['totals.hits_holiday_max_%i'%i].values/df_label['totals.pageviews_holiday_max_%i'%i].values\n",
    "        df_label['hits/pageviews_holiday_kernelMed_%i'%i] = df_label['totals.hits_holiday_kernelMed_%i'%i].values/df_label['totals.pageviews_holiday_kernelMed_%i'%i].values\n",
    "    df_label.fillna(0,inplace=True)\n",
    "    df_label = df_label.merge(df_train[['fullVisitorId']+cat_cols],on=['fullVisitorId'],how='left').fillna(-1)\n",
    "    \n",
    "    return df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time\n",
    "#时间窗\n",
    "# for windows in [14,28,42]:\n",
    "t2017 = date(2017,8,2)\n",
    "nday = 14\n",
    "# nday = windows\n",
    "all_data = []\n",
    "print(\"-\"*50+str(\"训练集构造\")+\"-\"*50)\n",
    "#构造训练集\n",
    "for i in range(nday,nday*(365//nday+1),nday):\n",
    "# for i in range(nday,nday*(2),nday):\n",
    "    delta = timedelta(days=i)\n",
    "    t_begin = t2017 - delta\n",
    "    print(t_begin)\n",
    "    df_data['day_gap'] = df_data['date'].apply(lambda x:dateGap(x,t_begin))\n",
    "    df_feature = df_data[df_data.day_gap<0].copy()\n",
    "#     df_label = df_data[(df_data.day_gap>=0)&(df_data.day_gap<nday)][['fullVisitorId','date','day_gap','totals.transactionRevenue']].copy()\n",
    "    df_label = df_data[(df_data.day_gap>=0)&(df_data.day_gap<nday)][\n",
    "                    ['fullVisitorId','date','visitStartTime','day_gap','totals.transactionRevenue']].copy()\n",
    "    data_temp = getFeatures(df_label,df_feature)\n",
    "    all_data.append(data_temp)\n",
    "\n",
    "train = pd.concat(all_data)\n",
    "print(\"-\"*50+str(\"测试集构造\")+\"-\"*50)\n",
    "\n",
    "#构造测试集合\n",
    "t_begin = date(2017,8,2)\n",
    "print(t_begin)\n",
    "df_label = df_data[df_data.is_test==True].copy()\n",
    "df_train = df_data[df_data.is_test!=True].copy()\n",
    "df_label['day_gap'] = df_label['date'].apply(lambda x:dateGap(x,t_begin))\n",
    "df_train['day_gap'] = df_train['date'].apply(lambda x:dateGap(x,t_begin))\n",
    "df_label = df_label[\n",
    "    ['fullVisitorId','date','visitStartTime','day_gap','totals.transactionRevenue']].copy()\n",
    "\n",
    "test = getFeatures(df_label,df_train)\n",
    "\n",
    "# 保存文件\n",
    "train.to_csv(\"./features/train_{}.csv\".format(nday),index=None)\n",
    "test.to_csv(\"./features/test.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as catb\n",
    "\n",
    "tr_features = [_f for _f in train.columns if _f not in excluded_cols]\n",
    "train_y = train['totals.transactionRevenue']\n",
    "\n",
    "def get_folds(df=None, n_splits=5):\n",
    "    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n",
    "    # Get sorted unique visitors\n",
    "    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n",
    "\n",
    "    # Get folds\n",
    "    folds = GroupKFold(n_splits=n_splits)\n",
    "    fold_ids = []\n",
    "    ids = np.arange(df.shape[0])\n",
    "    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n",
    "        fold_ids.append(\n",
    "            [\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n",
    "                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return fold_ids\n",
    "#GroupKFold 交叉验证输出\n",
    "def get_out_fold(model,x_train,y_train,x_test):\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    NFOLDS = 10\n",
    "    \n",
    "    folds = get_folds(df=x_train, n_splits=NFOLDS)\n",
    "    \n",
    "    oof_train_pred = np.zeros((ntrain,))\n",
    "    oof_test_pred = np.zeros((ntest,))\n",
    "    oof_test_pred_skf = np.empty((NFOLDS, ntest))\n",
    "    #针对不同的模型采用不同的训练方式\n",
    "    for i, (dev_index, val_index) in enumerate(folds):\n",
    "        x_dev = x_train[tr_features].iloc[dev_index]\n",
    "        y_dev = y_train.iloc[dev_index]\n",
    "        x_val = x_train[tr_features].iloc[val_index]\n",
    "        y_val = y_train.iloc[val_index]\n",
    "\n",
    "        model.fit(x_dev, y_dev,eval_set=[(x_dev,y_dev),(x_val,y_val)],early_stopping_rounds=500,verbose=100)\n",
    "    \n",
    "        oof_test_pred_skf[i, :] = model.predict(x_test[tr_features],num_iteration=model.best_iteration_)\n",
    "        oof_train_pred[val_index] = model.predict(x_val,num_iteration=model.best_iteration_)\n",
    "    \n",
    "    oof_test_pred_skf[oof_test_pred_skf<0] = 0\n",
    "    oof_test_pred[:] = np.expm1(oof_test_pred_skf).mean(axis=0)\n",
    "    oof_train_pred = np.expm1(oof_train_pred)\n",
    "    \n",
    "    print(\"-\"*50+str(\"model training done！\")+\"-\"*50)\n",
    "    \n",
    "    return model,oof_test_pred.reshape(-1, 1),oof_train_pred.reshape(-1,1)\n",
    "\n",
    "lgb_params={\n",
    "    'learning_rate': 0.03,\n",
    "    'objective':'regression',\n",
    "    'n_estimators':6000,\n",
    "    'metric':'rmse',\n",
    "    'num_leaves': 31,\n",
    "    'verbose': 1,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"random_state\":42,\n",
    "    'max_depth': 15,\n",
    "    'lambda_l2': 0.02,\n",
    "    'lambda_l1': 0.004,\n",
    "    'device': 'gpu',\n",
    "    'gpu_platform_id':1,\n",
    "    'gpu_device_id': 1,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.7,\n",
    "    'min_child_samples': 21\n",
    "}\n",
    "lgb_est = lgb.LGBMRegressor(**lgb_params)\n",
    "lgb_est,oof_lgb_test_pred,oof_lgb_train_pred = get_out_fold(lgb_est,train,train_y,test)\n",
    "# lgb_est.booster_.save_model('lgb_est_session.txt')\n",
    "\n",
    "lgb_train_pred = pd.DataFrame({\"fullVisitorId\":train['fullVisitorId'].values,\"lgb_train_pred\":oof_lgb_train_pred.reshape(-1,)})\n",
    "lgb_test_pred = pd.DataFrame({\"fullVisitorId\":test['fullVisitorId'].values,\"lgb_test_pred\":oof_lgb_test_pred.reshape(-1,)})\n",
    "# lgb_train_pred.to_csv('lgb_train_session_pred.csv',index=False)\n",
    "# lgb_test_pred.to_csv('lgb_test_session_pred.csv',index=False)\n",
    "\n",
    "#特征重要性分析\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "lgb.plot_importance(lgb_est,max_num_features=100, height=0.8, ax=ax)\n",
    "ax.grid(False)\n",
    "plt.title(\"LGBM - Feature Importance\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(clf,train_data,test_data,clf_name,class_num=1):\n",
    "    train=np.zeros((train_data.shape[0],class_num))\n",
    "    test=np.zeros((test_data.shape[0],class_num))\n",
    "    test_pre=np.empty((folds,test_data.shape[0],class_num))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf):\n",
    "        tr=train_data.iloc[train_index]\n",
    "        te=train_data.iloc[test_index]\n",
    "        #分别测试分数\n",
    "        te_1=te[te.day_gap<=6].copy()\n",
    "        te_2=te[te.day_gap>6].copy()\n",
    "        te_1_x=te_1.drop([\"totals.transactionRevenue\"], axis=1)\n",
    "        te_2_x=te_2.drop([\"totals.transactionRevenue\"], axis=1)\n",
    "        te_1_y=te_1[\"totals.transactionRevenue\"].values\n",
    "        te_2_y=te_2[\"totals.transactionRevenue\"].values\n",
    "        print(te_1.shape)\n",
    "        print(te_2.shape)\n",
    "\n",
    "        tr_x=tr.drop([\"totals.transactionRevenue\"], axis=1)\n",
    "        tr_y=tr['totals.transactionRevenue'].values\n",
    "        te_x=te.drop([\"totals.transactionRevenue\"], axis=1)\n",
    "        te_y=te['totals.transactionRevenue'].values\n",
    "\n",
    "        weight_train=weight_df.iloc[train_index]\n",
    "        weight_test=weight_df.iloc[test_index]\n",
    "\n",
    "        train_matrix = clf.Dataset(tr_x, label=tr_y,weight=weight_train[\"weight\"])\n",
    "        test_matrix = clf.Dataset(te_x, label=te_y,weight=weight_test[\"weight\"])\n",
    "\n",
    "        params = {\n",
    "            # Feiyang: 10. 把 7 改成了 8\n",
    "            'num_leaves': 2 ** 8 - 1,\n",
    "            'objective': 'regression_l2',\n",
    "            # Feiyang: 11. 把 8 改成了 9\n",
    "            'max_depth': 9,\n",
    "            'min_data_in_leaf': 50,\n",
    "            # Feiyang: 12. 把 0.01 改成了 0.007 并同时改了下面的 Num_round 和 early_stopping_rounds\n",
    "            'learning_rate': 0.007,\n",
    "            'feature_fraction': 0.6,\n",
    "            # Feiyang: 13. 把 0.75 改成了 0.8\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 1,\n",
    "            'metric': 'rmse',\n",
    "            'device': 'gpu',\n",
    "            'gpu_platform_id':1,\n",
    "            'gpu_device_id': 1,\n",
    "            'num_threads': 4,\n",
    "            'seed': 2018,\n",
    "        }\n",
    "\n",
    "        num_round = 6000\n",
    "        early_stopping_rounds = 500\n",
    "        if test_matrix:\n",
    "            model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                              early_stopping_rounds=early_stopping_rounds\n",
    "                              )\n",
    "            pre= model.predict(te_x,num_iteration=model.best_iteration).reshape((te_x.shape[0],1))\n",
    "            train[test_index]=pre\n",
    "            test_pre[i, :]= model.predict(test_data, num_iteration=model.best_iteration).reshape((test_data.shape[0],1))\n",
    "            pre_1=model.predict(te_1_x,num_iteration=model.best_iteration).reshape((te_1_x.shape[0],1))\n",
    "            pre_2=model.predict(te_2_x,num_iteration=model.best_iteration).reshape((te_2_x.shape[0],1))\n",
    "            cv_scores.append((mean_squared_error(te_y, pre)**0.5,mean_squared_error(te_1_y, pre_1)**0.5,mean_squared_error(te_2_y, pre_2)**0.5))\n",
    "\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "\n",
    "    score_split=(str(round(np.mean([i[0] for i in cv_scores]),6)),str(round(np.mean([i[1] for i in cv_scores]),6)),str(round(np.mean([i[2] for i in cv_scores]),6)))\n",
    "    with open(\"score_cv.txt\", \"a\") as f:\n",
    "        f.write(\"%s now score is:\" % clf_name + str(cv_scores) + \"\\n\")\n",
    "        f.write(\"%s_score_mean:\"%clf_name+str(np.mean(cv_scores))+\"\\n\")\n",
    "        f.write(\"score_split:\"+str(score_split)+\"\\n\")\n",
    "\n",
    "    return train.reshape(-1,class_num),test.reshape(-1,class_num),score_split\n",
    "\n",
    "\n",
    "def lgb(train, valid):\n",
    "    xgb_train, xgb_test,cv_scores = stacking(lightgbm,train,valid,\"lgb\")\n",
    "    return xgb_train, xgb_test,cv_scores\n",
    "\n",
    "import lightgbm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = 5\n",
    "seed = 2018\n",
    "\n",
    "#生成数据\n",
    "train_data = train.drop([\"air_store_id\",\"hpg_store_id\",\"visit_date\"], axis=1)\n",
    "test_data = test.drop([\"air_store_id\",\"hpg_store_id\",\"visit_date\"], axis=1)\n",
    "\n",
    "weight_df=train[[\"day_gap\"]].copy()\n",
    "weight_df[\"weight\"]=weight_df[\"day_gap\"].apply(lambda x: 1 if x<=6 else 1)\n",
    "\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=seed).split(train)\n",
    "lgb_train, lgb_test,m = lgb(train_data,test_data)\n",
    "\n",
    "#生成线下\n",
    "train[\"visitors_pre\"]=lgb_train\n",
    "score_result=mean_squared_error(train[\"visitors\"], train[\"visitors_pre\"])**0.5\n",
    "train[\"visitors\"] = np.clip(np.expm1(train[\"visitors\"]), 0, 1000)\n",
    "train[\"visitors_pre\"] = np.clip(np.expm1(train[\"visitors_pre\"]), 0, 1000)\n",
    "train[[\"air_store_id\",\"visit_date\",\"visitors\",\"visitors_pre\"]].to_csv(\"./offline/offline_cv_%s_%s_%s.csv\"%m,index=None)\n",
    "with open(\"score_cv.txt\", \"a\") as f:\n",
    "    f.write(\"result score is:\" + str(score_result) + \"\\n\")\n",
    "#生成提交\n",
    "df_test[\"visitors\"]=lgb_test\n",
    "df_test[\"visitors\"] = np.clip(np.expm1(df_test[\"visitors\"]), 0, 1000)\n",
    "df_test[[\"id\",\"visitors\"]].to_csv(\"./submission/sub_woo_cv_%s_weight_%s_%s_%s.csv\"%(slip,m[0],m[1],m[2]),index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_totals_feature(df):\n",
    "    '''\n",
    "        totals点击量处理\n",
    "    '''\n",
    "    df['totals.pageviews'] = df['totals.pageviews'].fillna(1.0)\n",
    "    df['totals.newVisits'] = df['totals.newVisits'].fillna(0)\n",
    "    df['totals.bounces'] = df['totals.bounces'].fillna(0)\n",
    "    \n",
    "    #用户每天，每周，每月的访问量\n",
    "    df['sum_visits_per_day_visitor'] = df.groupby(['fullVisitorId','month','day'])['totals.visits'].transform('sum')\n",
    "    df['sum_visits_per_week_visitor'] = df.groupby(['fullVisitorId','month','week'])['totals.visits'].transform('sum')\n",
    "    df['sum_visits_per_month_visitor'] = df.groupby(['fullVisitorId','month'])['totals.visits'].transform('sum')\n",
    "    \n",
    "    #用户每天，每周，每月的点击量\n",
    "    df['sum_hits_per_day_visitor'] = df.groupby(['fullVisitorId','month','day'])['totals.hits'].transform('sum')\n",
    "    df['sum_hits_per_week_visitor'] = df.groupby(['fullVisitorId','month','week'])['totals.hits'].transform('sum')\n",
    "    df['sum_hits_per_month_visitor'] = df.groupby(['fullVisitorId','month'])['totals.hits'].transform('sum')\n",
    "    \n",
    "    #用户每天，每周，每月的网页浏览量\n",
    "    df['sum_pageviews_per_day_visitor'] = df.groupby(['fullVisitorId','month','day'])['totals.pageviews'].transform('sum')\n",
    "    df['sum_pageviews_per_week_visitor'] = df.groupby(['fullVisitorId','month','week'])['totals.pageviews'].transform('sum')\n",
    "    df['sum_pageviews_per_month_visitor'] = df.groupby(['fullVisitorId','month'])['totals.pageviews'].transform('sum')\n",
    "    \n",
    "    #用户每个pageView产生的点击量hits/pageviews\n",
    "    df['hits/pageViews_per_day_visitor'] = df['sum_hits_per_day_visitor'].values/df['sum_pageviews_per_day_visitor'].values\n",
    "    df['hits/pageViews_per_week_visitor'] = df['sum_hits_per_week_visitor'].values/df['sum_pageviews_per_week_visitor'].values\n",
    "    df['hits/pageviews_per_month_visitor'] =df['sum_hits_per_month_visitor'].values/df['sum_pageviews_per_month_visitor'].values\n",
    "    \n",
    "    #用户每天，每周，每月的session量\n",
    "    df['sum_visitNumber_per_day_visitor'] = df.groupby(['fullVisitorId','month','day'])['visitNumber'].transform('sum')\n",
    "    df['sum_visitNumber_per_week_visitor'] = df.groupby(['fullVisitorId','month','week'])['visitNumber'].transform('sum')\n",
    "    df['sum_visitNumber_per_month_visitor'] = df.groupby(['fullVisitorId','month'])['visitNumber'].transform('sum')\n",
    "    \n",
    "    #平均session量\n",
    "    df['mean_vistiNumber_per_day_visitor'] = df.groupby(['fullVisitorId','month','day'])['visitNumber'].transform('mean')\n",
    "    df['mean_visitNumebr_per_week_visitor']= df.groupby(['fullVisitorId','month','week_day'])['visitNumber'].transform('mean')\n",
    "    df['mean_visitNumber_per_month_visitor'] = df.groupby(['fullVisitorId','month'])['visitNumber'].transform('mean')\n",
    "    \n",
    "    #每次session产生的pageviews pageview/visitNumber\n",
    "    df['pageviews/visitNumber_per_day_visitor'] = df['sum_pageviews_per_day_visitor'].values/df['sum_visitNumber_per_day_visitor'].values\n",
    "    df['pageviews/visitNumber_per_week_visitor'] = df['sum_pageviews_per_week_visitor'].values/df['sum_visitNumber_per_week_visitor'].values\n",
    "    df['pageviews/visitNumber_per_month_visitor'] = df['sum_pageviews_per_month_visitor'].values/df['sum_visitNumber_per_month_visitor'].values\n",
    "    \n",
    "    df['mean.hits.per.day'] = df.groupby(['month','day'])['totals.hits'].transform('mean')\n",
    "    df['sum.hits.per.day'] = df.groupby(['month','day'])['totals.hits'].transform('sum')\n",
    "    \n",
    "    df['mean.hits.per.month'] = df.groupby(['month'])['totals.hits'].transform('mean')\n",
    "    df['sum.hits.per.month'] = df.groupby(['month'])['totals.hits'].transform('sum')\n",
    "    \n",
    "    df['mean.hits.per.week_day'] = df.groupby(['month','week_day'])['totals.hits'].transform('mean')\n",
    "    df['sum.hits.per.week_day'] = df.groupby(['month','week_day'])['totals.hits'].transform('sum')\n",
    "    \n",
    "    df['mean.pageviews.per.day'] = df.groupby(['month','day'])['totals.pageviews'].transform('mean')\n",
    "    df['sum.pageviews.per.day'] = df.groupby(['month','day'])['totals.pageviews'].transform('sum')\n",
    "    \n",
    "    df['mean.pageviews.per.month'] = df.groupby(['month'])['totals.pageviews'].transform('mean')\n",
    "    df['sum.pageviews.per.month'] = df.groupby(['month'])['totals.pageviews'].transform('sum')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_data = get_totals_feature(df_data)    \n",
    "\n",
    "print(df_data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
